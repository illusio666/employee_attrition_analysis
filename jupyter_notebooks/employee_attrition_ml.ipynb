{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62341c2a",
   "metadata": {},
   "source": [
    "# Using machine learning to identity clusters of 'at risk' employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "988af030",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Import necessary libraries\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib  # for saving an ml model\n",
    "\n",
    "# Robust data-file finder and loader (uses a relative path for reading)\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf06867",
   "metadata": {},
   "source": [
    "Identifying causes of attrition and enabling the business to identify different groups of 'at risk' employees is key. \n",
    "\n",
    "Steps 1-5 - This notebook will create a model that will assess the 1400 rows of data currently available (split by 'train' and 'test' groups) to identify if there are logical groupings of exited staff, to enable future departures to be anticipated and, if desired, attempts made to retain.\n",
    "\n",
    "Step 6 - we will then look to cluster the data to identify any features that would help the business devise a retention strategy for distinct groups of leavers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce3933",
   "metadata": {},
   "source": [
    "### Step 1 - import the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4534960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 39 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Age                           1470 non-null   int64  \n",
      " 1   Attrition                     1470 non-null   object \n",
      " 2   BusinessTravel                1470 non-null   object \n",
      " 3   DailyRate                     1470 non-null   int64  \n",
      " 4   Department                    1470 non-null   object \n",
      " 5   DistanceFromHome              1470 non-null   int64  \n",
      " 6   Education                     1470 non-null   object \n",
      " 7   EducationField                1470 non-null   object \n",
      " 8   EnvironmentSatisfaction       1470 non-null   object \n",
      " 9   Gender                        1470 non-null   object \n",
      " 10  HourlyRate                    1470 non-null   int64  \n",
      " 11  JobInvolvement                1470 non-null   object \n",
      " 12  JobLevel                      1470 non-null   int64  \n",
      " 13  JobRole                       1470 non-null   object \n",
      " 14  JobSatisfaction               1470 non-null   object \n",
      " 15  MaritalStatus                 1470 non-null   object \n",
      " 16  MonthlyIncome                 1470 non-null   int64  \n",
      " 17  MonthlyRate                   1470 non-null   int64  \n",
      " 18  NumCompaniesWorked            1470 non-null   int64  \n",
      " 19  OverTime                      1470 non-null   object \n",
      " 20  PercentSalaryHike             1470 non-null   int64  \n",
      " 21  PerformanceRating             1470 non-null   object \n",
      " 22  RelationshipSatisfaction      1470 non-null   object \n",
      " 23  StockOptionLevel              1470 non-null   int64  \n",
      " 24  TotalWorkingYears             1470 non-null   int64  \n",
      " 25  TrainingTimesLastYear         1470 non-null   int64  \n",
      " 26  WorkLifeBalance               1470 non-null   object \n",
      " 27  YearsAtCompany                1470 non-null   int64  \n",
      " 28  YearsInCurrentRole            1470 non-null   int64  \n",
      " 29  YearsSinceLastPromotion       1470 non-null   int64  \n",
      " 30  YearsWithCurrManager          1470 non-null   int64  \n",
      " 31  RoleTenureRatio               1470 non-null   float64\n",
      " 32  ManagerTenureRatio            1470 non-null   float64\n",
      " 33  IncomeTenureRatio             1470 non-null   float64\n",
      " 34  IncomeTotalWorkRatio          1470 non-null   float64\n",
      " 35  RoleTenureRatio_was_nan       1470 non-null   int64  \n",
      " 36  ManagerTenureRatio_was_nan    1470 non-null   int64  \n",
      " 37  IncomeTenureRatio_was_nan     1470 non-null   int64  \n",
      " 38  IncomeTotalWorkRatio_was_nan  1470 non-null   int64  \n",
      "dtypes: float64(4), int64(20), object(15)\n",
      "memory usage: 448.0+ KB\n",
      "None\n",
      "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
      "1   49        No  Travel_Frequently        279  Research & Development   \n",
      "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
      "3   33        No  Travel_Frequently       1392  Research & Development   \n",
      "4   27        No      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome        Education EducationField EnvironmentSatisfaction  \\\n",
      "0                 1        2-College  Life Sciences                2-Medium   \n",
      "1                 8  1-Below College  Life Sciences                  3-High   \n",
      "2                 2        2-College          Other             4-Very High   \n",
      "3                 3         4-Master  Life Sciences             4-Very High   \n",
      "4                 2  1-Below College        Medical                   1-Low   \n",
      "\n",
      "   Gender  ...  YearsSinceLastPromotion YearsWithCurrManager  RoleTenureRatio  \\\n",
      "0  Female  ...                        0                    5             0.67   \n",
      "1    Male  ...                        1                    7             0.70   \n",
      "2    Male  ...                        0                    0             0.00   \n",
      "3  Female  ...                        3                    0             0.88   \n",
      "4    Male  ...                        2                    2             1.00   \n",
      "\n",
      "  ManagerTenureRatio IncomeTenureRatio IncomeTotalWorkRatio  \\\n",
      "0               0.83            998.83               749.12   \n",
      "1               0.70            513.00               513.00   \n",
      "2               0.00              0.00               298.57   \n",
      "3               0.00            363.62               363.62   \n",
      "4               1.00           1734.00               578.00   \n",
      "\n",
      "   RoleTenureRatio_was_nan  ManagerTenureRatio_was_nan  \\\n",
      "0                        0                           0   \n",
      "1                        0                           0   \n",
      "2                        0                           0   \n",
      "3                        0                           0   \n",
      "4                        0                           0   \n",
      "\n",
      "   IncomeTenureRatio_was_nan IncomeTotalWorkRatio_was_nan  \n",
      "0                          0                            0  \n",
      "1                          0                            0  \n",
      "2                          0                            0  \n",
      "3                          0                            0  \n",
      "4                          0                            0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data using an explicit relative path\n",
    "df = pd.read_csv('../Data files/HR_Attrition_Cleaned.csv')\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69513b",
   "metadata": {},
   "source": [
    "### Step 2 - Data preparation for ML\n",
    "\n",
    "Next, as we're working with a mix of numeric and string columns, we'll identify the data types in readiness to encoding. But our target column (Attrition) needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cca4b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"Attrition\"\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "# Remove target from both lists if present\n",
    "numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "categorical_cols = [col for col in categorical_cols if col != target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24820435",
   "metadata": {},
   "source": [
    "Now we need to split our data into a training set and then a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af75ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1029, 38) (1029,)\n",
      "Test shape: (441, 38) (441,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(target_col, axis=1)\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=101\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e8f32",
   "metadata": {},
   "source": [
    "Now we need to pre-process our data to allow the pipeline to handle those different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eaef6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", StandardScaler(), numeric_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ada56c",
   "metadata": {},
   "source": [
    "### Step 3 - build pipeline, create model\n",
    "\n",
    "Now the pipeline model can be built. We are using Random Forest because it handles mixed data sets, can identify complex interactions (ydata-profiling already identified that there's no single strong correlation for Attrition) and can rank importance. It is also more appropriate for 'imbalanced' data sets, like attrition, where values are more likely to be in the 'still employed' side of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f32cac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff1f22",
   "metadata": {},
   "source": [
    "Now we train the model using the 'train' data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "addd1737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 0.8526077097505669\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model score:\", pipeline.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105251b",
   "metadata": {},
   "source": [
    "While a score of 0.85 indicates a strong accuracy rate, this can sometimes be misleading if there is not an even split in the data. So we need to understand if the prediction of 'Yes' to attrition is strong, rather than a mean score across the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12806def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Attrition\n",
       "No     0.838776\n",
       "Yes    0.161224\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the split of data between yes and no for attrition\n",
    "df[\"Attrition\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab409cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.86      0.98      0.92       371\n",
      "         Yes       0.65      0.16      0.25        70\n",
      "\n",
      "    accuracy                           0.85       441\n",
      "   macro avg       0.75      0.57      0.59       441\n",
      "weighted avg       0.83      0.85      0.81       441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check accuracy for each of the two groups\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a474",
   "metadata": {},
   "source": [
    "The model is right 65% of the time when it predicts attrition to be Yes (precision) but it's only picking up 16% of the actual leavers (recall).\n",
    "\n",
    "So we need to try to fine tune the model to improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d03a31",
   "metadata": {},
   "source": [
    "### Step 4 - improve outcomes\n",
    "\n",
    "We have several options to refine the model. We'll look at two of those options:\n",
    "\n",
    "1) weight the mistakes more strongly, to take attrition more seriously, and\n",
    "2) change the balance of Yes and No cases (create some fake-but-similar Yes rows or remove some of the No rows).\n",
    "\n",
    "We'll do both of these, with 2) first to improve the training data, followed by 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3f3279e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 0: Encode categorical columns in X\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in X_encoded.select_dtypes(include=\"object\").columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35db5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split encoded data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c05aa6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build pipeline with SMOTE and class-weighted model\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),  # Optional\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67ee86e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "                                        random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()), (&#x27;smote&#x27;, SMOTE(random_state=42)),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestClassifier(class_weight=&#x27;balanced&#x27;,\n",
       "                                        random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('smote', SMOTE(random_state=42)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(class_weight='balanced',\n",
       "                                        random_state=42))])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d22c8d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.86      0.96      0.91       247\n",
      "         Yes       0.48      0.21      0.29        47\n",
      "\n",
      "    accuracy                           0.84       294\n",
      "   macro avg       0.67      0.58      0.60       294\n",
      "weighted avg       0.80      0.84      0.81       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6b57e",
   "metadata": {},
   "source": [
    "Results are still not strong, and while recall has gone up, precision has decreased.\n",
    "\n",
    "We will reduce the confidence threshold to see if this improves our outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ac2f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.93      0.81      0.87       247\n",
      "         Yes       0.41      0.68      0.51        47\n",
      "\n",
      "    accuracy                           0.79       294\n",
      "   macro avg       0.67      0.75      0.69       294\n",
      "weighted avg       0.85      0.79      0.81       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 5: Adjust classification threshold\n",
    "# Get predicted probabilities for the “Yes” class\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Lower threshold from 0.5 to 0.3 (or test multiple)\n",
    "y_pred_thresh = (y_proba >= 0.3).astype(int)\n",
    "\n",
    "# Map numeric predictions back to original labels, to prevent TypeError that was being received\n",
    "y_pred_labels = np.where(y_pred_thresh == 1, \"Yes\", \"No\")\n",
    "\n",
    "# Evaluate\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8da009",
   "metadata": {},
   "source": [
    "It's now picking up 68% of the attrition Yes cases, but when it predicts Yes, it's only right 41% of the time.\n",
    "\n",
    "While 'err on the side of caution' is probably acceptable from an employee retention perspective, we will also run a logistic regression to see if that provides stronger results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6d59c",
   "metadata": {},
   "source": [
    "### Step 5 - run and compare alternative model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad9c2f",
   "metadata": {},
   "source": [
    "Logistic regression is another model that might be used to identify leavers, so we will run this to see if the results are more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63e07163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.94      0.75      0.84       247\n",
      "         Yes       0.37      0.77      0.50        47\n",
      "\n",
      "    accuracy                           0.75       294\n",
      "   macro avg       0.66      0.76      0.67       294\n",
      "weighted avg       0.85      0.75      0.78       294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 0: Encode categorical features (if not already done)\n",
    "# Use X_encoded from earlier\n",
    "\n",
    "# Step 1: Split encoded data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Build pipeline with SMOTE and weighted Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Step 3: Fit and evaluate\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b64db0",
   "metadata": {},
   "source": [
    "Conclusion: while the precision of the Logistic regression model isn't high, it is better to capture extra (not at risk) staff when deploying a retention strategy but ensuring we maximise the actual leavers that are captured. So this model gives slightly better results when it comes to accurately identifying leavers.\n",
    "\n",
    "Now we can save this model so it can be deployed against future tranches of the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6199b987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_attrition_model.pkl']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the pipeline to a file\n",
    "joblib.dump(pipeline, \"logistic_attrition_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d572e",
   "metadata": {},
   "source": [
    "### Clustering - finding structure in the attrition data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba12715",
   "metadata": {},
   "source": [
    "We'll now look at whether there are any patterns in our attrition data, to see if this provides insight on how retention strategies might be developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ee69d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition        No       Yes\n",
      "Cluster                      \n",
      "0          0.910000  0.090000\n",
      "1          0.896296  0.103704\n",
      "2          0.825581  0.174419\n",
      "3          0.694969  0.305031\n",
      "4          0.879070  0.120930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rache\\.vscode\\employee_attrition_analysis\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Scale features (excluding target)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Step 2: Run KMeans (try 5 clusters)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 3: Add cluster and attrition info to a new DataFrame\n",
    "cluster_df = pd.DataFrame(X_encoded)\n",
    "cluster_df[\"Cluster\"] = cluster_labels\n",
    "cluster_df[\"Attrition\"] = y.values  # original string labels\n",
    "\n",
    "# Step 4: Profile clusters\n",
    "cluster_summary = cluster_df.groupby(\"Cluster\")[\"Attrition\"].value_counts(normalize=True).unstack().fillna(0)\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa9255",
   "metadata": {},
   "source": [
    "| K-Means clusters              | Outcomes                                      | Summary                          | \n",
    "|----------------------|------------------------------------------------------|--------------------------------------------------------|\n",
    "| 3 cluster| <img src=\"KMeans 3 cluster.png\" width=\"60%\">| One group above average for total data set. Group1 is 'very safe  |\n",
    "| 4 cluster|  <img src=\"KMeans 4 cluster.png\" width=\"50%\">| Two groups above average for total data set |\n",
    "| 5 cluster|  <img src=\"KMeans 5 cluster.png\" width=\"60%\">| Two groups above average for total data set |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f174e1",
   "metadata": {},
   "source": [
    "Having run 3,4 and 5 cluster tests, 5 clusters gives us two groups which would be worth assessing for possible retention strategy in the Yes group (dataset average for attrition is 16.1%). While 4 clusters also gives us two groups, the 5-cluster grouping will be slightly more tailored by virtue of the additional split.\n",
    "\n",
    "Group3 contains 30% employees who leave, and group2 is made up of 17% leavers. So Group3 is our highest priority but no harm considering a strategy for Group2 who are marginally abovew average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7af8b6",
   "metadata": {},
   "source": [
    "Now we need to look at what common features cause these groups to be clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "935bca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Averages for Clusters 2 and 3:\n",
      " Cluster                                  2             3\n",
      "Age                              35.563953     29.754717\n",
      "DailyRate                       795.290698    811.503145\n",
      "DistanceFromHome                  9.959302      8.503145\n",
      "HourlyRate                       65.186047     66.584906\n",
      "JobLevel                          1.709302      1.279874\n",
      "MonthlyIncome                  4889.052326   3455.443396\n",
      "MonthlyRate                   13820.232558  14676.128931\n",
      "NumCompaniesWorked                2.482558      1.716981\n",
      "PercentSalaryHike                21.715116     14.632075\n",
      "StockOptionLevel                  0.843023      0.641509\n",
      "TotalWorkingYears                 9.250000      4.100629\n",
      "TrainingTimesLastYear             2.767442      2.867925\n",
      "YearsAtCompany                    6.226744      2.515723\n",
      "YearsInCurrentRole                4.273256      1.248428\n",
      "YearsSinceLastPromotion           1.831395      0.575472\n",
      "YearsWithCurrManager              4.191860      1.066038\n",
      "RoleTenureRatio                   0.635233      0.377296\n",
      "ManagerTenureRatio                0.627558      0.315157\n",
      "IncomeTenureRatio               981.980465   1429.944686\n",
      "IncomeTotalWorkRatio            612.056919   1138.963113\n",
      "RoleTenureRatio_was_nan           0.000000      0.000000\n",
      "ManagerTenureRatio_was_nan        0.000000      0.000000\n",
      "IncomeTenureRatio_was_nan         0.000000      0.000000\n",
      "IncomeTotalWorkRatio_was_nan      0.000000      0.000000\n",
      "\n",
      "Attrition Rates:\n",
      " Attrition        No       Yes\n",
      "Cluster                      \n",
      "2          0.825581  0.174419\n",
      "3          0.694969  0.305031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rache\\.vscode\\employee_attrition_analysis\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Step 2: Run KMeans with 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 3: Build full cluster DataFrame\n",
    "cluster_df = pd.DataFrame(X_encoded.copy())\n",
    "cluster_df[\"Cluster\"] = cluster_labels\n",
    "cluster_df[\"Attrition\"] = y.values  # original string labels\n",
    "\n",
    "# Step 4: Filter high-risk clusters\n",
    "high_risk_clusters = cluster_df[cluster_df[\"Cluster\"].isin([2, 3])]\n",
    "\n",
    "# Step 5: Restrict to numeric columns only\n",
    "numeric_cols = high_risk_clusters.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "profile_summary = high_risk_clusters.groupby(\"Cluster\")[numeric_cols].mean().T\n",
    "print(\"Feature Averages for Clusters 2 and 3:\\n\", profile_summary)\n",
    "\n",
    "# Step 6: Attrition breakdown\n",
    "attrition_counts = high_risk_clusters.groupby(\"Cluster\")[\"Attrition\"].value_counts(normalize=True).unstack().fillna(0)\n",
    "print(\"\\nAttrition Rates:\\n\", attrition_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2c1374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster vs Dataset Averages:\n",
      "                                          2             3   Dataset_Avg\n",
      "Age                              35.563953     29.754717     36.923810\n",
      "DailyRate                       795.290698    811.503145    802.485714\n",
      "DistanceFromHome                  9.959302      8.503145      9.192517\n",
      "HourlyRate                       65.186047     66.584906     65.891156\n",
      "JobLevel                          1.709302      1.279874      2.063946\n",
      "MonthlyIncome                  4889.052326   3455.443396   6502.931293\n",
      "MonthlyRate                   13820.232558  14676.128931  14313.103401\n",
      "NumCompaniesWorked                2.482558      1.716981      2.693197\n",
      "PercentSalaryHike                21.715116     14.632075     15.209524\n",
      "StockOptionLevel                  0.843023      0.641509      0.793878\n",
      "TotalWorkingYears                 9.250000      4.100629     11.279592\n",
      "TrainingTimesLastYear             2.767442      2.867925      2.799320\n",
      "YearsAtCompany                    6.226744      2.515723      7.008163\n",
      "YearsInCurrentRole                4.273256      1.248428      4.229252\n",
      "YearsSinceLastPromotion           1.831395      0.575472      2.187755\n",
      "YearsWithCurrManager              4.191860      1.066038      4.123129\n",
      "RoleTenureRatio                   0.635233      0.377296      0.578755\n",
      "ManagerTenureRatio                0.627558      0.315157      0.559122\n",
      "IncomeTenureRatio               981.980465   1429.944686   1505.816279\n",
      "IncomeTotalWorkRatio            612.056919   1138.963113    706.865007\n",
      "RoleTenureRatio_was_nan           0.000000      0.000000      0.000000\n",
      "ManagerTenureRatio_was_nan        0.000000      0.000000      0.000000\n",
      "IncomeTenureRatio_was_nan         0.000000      0.000000      0.000000\n",
      "IncomeTotalWorkRatio_was_nan      0.000000      0.000000      0.000000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute dataset-wide averages\n",
    "dataset_avg = pd.DataFrame(X_encoded.select_dtypes(include=[\"int64\", \"float64\"]).mean(), columns=[\"Dataset_Avg\"])\n",
    "\n",
    "# Step 2: Get cluster averages for clusters 2 and 3\n",
    "cluster_avg = high_risk_clusters.groupby(\"Cluster\")[dataset_avg.index].mean().T\n",
    "\n",
    "# Step 3: Concatenate for comparison\n",
    "comparison_table = pd.concat([cluster_avg, dataset_avg], axis=1)\n",
    "print(\"Cluster vs Dataset Averages:\\n\", comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7895792d",
   "metadata": {},
   "source": [
    "So this is telling us a couple of things for our strategy development:\n",
    "\n",
    "- Group 3 (highest risk) are relatively young (avg 29.8), low level roles (avg 1.28) and only been at the company an avg of 2.5yrs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
